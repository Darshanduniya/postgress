from datetime import datetime
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
import psycopg2

# Define the default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 8, 23),
    'depends_on_past': False,
    'retries': 1,
}

# Define the DAG
dag = DAG(
    'execute_postgres_query',
    default_args=default_args,
    schedule_interval=None,  # Set the interval at which the DAG runs (None for manual execution)
    catchup=False,  # Do not catch up on historical dates
)

# Define a function to execute the PostgreSQL query
def execute_postgres_query():
    # Connect to the PostgreSQL database
    conn = psycopg2.connect(
        host='your_database_host',
        dbname='your_database_name',
        user='your_username',
        password='your_password'
    )
    
    # Create a cursor
    cursor = conn.cursor()
    
    # Define the query
    query = "SELECT industry_name, status_flag, model_name FROM darshan_table;"
    
    # Execute the query
    cursor.execute(query)
    
    # Fetch all rows
    rows = cursor.fetchall()
    
    # Print the results
    for row in rows:
        print(row)
    
    # Close the cursor and connection
    cursor.close()
    conn.close()

# Define a PythonOperator to execute the query
execute_query_task = PythonOperator(
    task_id='execute_query_task',
    python_callable=execute_postgres_query,
    dag=dag,
)

# Set task dependencies
execute_query_task

